# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kTrXf76flIHWZJ6dtYgGWPn9HqjNniRL
"""

import numpy as np
import gym
from gym import spaces
import random

# Mock network load data (simulating traffic patterns for training)
np.random.seed(42)
network_load_data = np.random.randint(1, 100, size=1000)  # Traffic load values between 1 and 100

# Define custom environment for 5G energy optimization
class EnergyOptimizationEnv(gym.Env):
    def __init__(self):  # Corrected method name
        super(EnergyOptimizationEnv, self).__init__()
        self.action_space = spaces.Discrete(2)  # 0 = Normal Mode, 1 = Power-Saving Mode
        self.observation_space = spaces.Box(low=1, high=100, shape=(1,), dtype=np.int32)
        self.current_step = 0
        self.total_energy_consumption = 0
        self.energy_consumed_normal = 10  # Energy in normal mode
        self.energy_consumed_power_save = 3  # Energy in power-saving mode

    def reset(self):
        self.current_step = 0
        self.total_energy_consumption = 0
        return [network_load_data[self.current_step]]

    def step(self, action):
        load = network_load_data[self.current_step]
        if action == 1 and load < 20:  # Power-saving mode if load is low
            energy_used = self.energy_consumed_power_save
            reward = 10  # Positive reward for saving energy
        else:
            energy_used = self.energy_consumed_normal
            reward = -1 if load > 80 and action == 1 else 0  # Penalty if high load in power-saving

        self.total_energy_consumption += energy_used
        self.current_step += 1
        done = self.current_step >= len(network_load_data) - 1
        next_state = [network_load_data[self.current_step]] if not done else [0]

        return next_state, reward, done, {}

    def render(self, mode='human'):
        print(f"Step: {self.current_step}, Total Energy Consumption: {self.total_energy_consumption}")

# Q-learning algorithm
def q_learning(env, episodes=1000, alpha=0.1, gamma=0.95, epsilon=1.0, epsilon_decay=0.995):
    q_table = np.zeros((101, env.action_space.n))  # State-action values
    for episode in range(episodes):
        state = env.reset()
        done = False
        while not done:
            state_idx = state[0]
            if random.uniform(0, 1) < epsilon:  # Epsilon-greedy strategy
                action = env.action_space.sample()
            else:
                action = np.argmax(q_table[state_idx])

            next_state, reward, done, _ = env.step(action)
            next_state_idx = next_state[0]
            q_value = q_table[state_idx, action]
            best_next_q = np.max(q_table[next_state_idx])

            # Update Q-value
            q_table[state_idx, action] = (1 - alpha) * q_value + alpha * (reward + gamma * best_next_q)

            state = next_state

        epsilon *= epsilon_decay  # Decay epsilon after each episode

    return q_table

# Train the model
env = EnergyOptimizationEnv()
q_table = q_learning(env)

# Print Q-table
print("Training complete. Q-table values:")
print(q_table)

# Test the trained model
state = env.reset()
done = False
while not done:
    state_idx = state[0]
    action = np.argmax(q_table[state_idx])  # Choose the best action based on the Q-table
    state, reward, done, _ = env.step(action)
    env.render()

# Plotting multiple graphs
plt.figure(figsize=(16, 12))

# 1. Total Energy Consumption Over Steps
plt.subplot(2, 2, 1)
plt.plot(steps, energy_consumptions, label="Total Energy Consumption", color="blue")
plt.title("Total Energy Consumption Over Steps")
plt.xlabel("Steps")
plt.ylabel("Energy Consumption")
plt.ylim(0, max(energy_consumptions) + 10)  # Adjust y-axis range
plt.xlim(0, len(steps))  # Adjust x-axis range
plt.legend()
plt.grid()

# 2. Rewards Over Steps
plt.subplot(2, 2, 2)
plt.plot(steps, rewards, label="Rewards", color="green")
plt.title("Rewards Over Steps")
plt.xlabel("Steps")
plt.ylabel("Reward")
plt.legend()
plt.grid()

# 3. Actions Taken Over Steps
plt.subplot(2, 2, 3)
plt.step(steps, actions, label="Actions (0 = Normal, 1 = Power-Saving)", color="orange", where="post")
plt.title("Actions Taken Over Steps")
plt.xlabel("Steps")
plt.ylabel("Action")
plt.yticks([0, 1], ["Normal", "Power-Saving"])
plt.legend()
plt.grid()

# 4. Network Load vs. Energy Consumption
plt.subplot(2, 2, 4)
plt.scatter(loads, energy_consumptions, label="Energy Consumption by Load", color="purple", alpha=0.6)
plt.title("Network Load vs. Energy Consumption")
plt.xlabel("Network Load")
plt.ylabel("Energy Consumption")
plt.legend()
plt.grid()

# Show the plots
plt.tight_layout()
plt.show()

